
import pandas as pd

import numpy as np

import nltk

from nltk.tokenize import sent_tokenize

from nltk.tokenize import word_tokenize

import nltk.corpus

from nltk.corpus import wordnet, stopwords

from fuzzywuzzy import fuzz, process

import hashlib

import gensim

from gensim.summarization import summarize

from gensim.summarization import keywords

import array

#establishkeywords
#keyword index is initially established by the developer/user

inputwords = ["politics", "economics", "money"]
searchwords = [inputwords]

print("You have identified the following of interest:")
print(inputwords)
print("----------------------------------------------------------")

#Importing the nltk wordnet, will give suggested alternate words if spelling off and syn/ant

synonyms = []
antonyms = []

for k in inputwords:
    for syns in wordnet.synsets(k):
        #print(syns[0].name())
        #print(syns[0].lemmas()[0].name())
        #print(syns[0].definition())
        #print(syns[0].examples())
        for l in syns.lemmas():
            synonyms.append(l.name())
  
            if l.antonyms():
                antonyms.append(l.antonyms()[0].name())
    searchwords = [searchwords + synonyms + antonyms]        
    
    print("----------------------------------------------------------")

print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("I've taken the liberty to also search these common synonyms and antonyms: ")
print(searchwords)
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")


#should consider a pandas input here to make sure all variants/spellings captured

#open file and tokenize
#To Do: Open and read all files in a folder and tokenize instead of one file at a time
f=open("Text/natsrvc.txt").read()
sentences=sent_tokenize(f)

for s in searchwords:
    term = str(s)
    print("---break---", )
    fuzzyset = process.extractBests(term, sentences, score_cutoff=40, limit=100)
    print(fuzzyset)
    print("---break---")
    #to consider: grab sentence before and sentence after in + to just match here

meaningful_text = str(fuzzyset)

# to do: clean up output to get rid of line breaks

#due to trouble with gensim, considering use of stopwords below is placeholder
#stopWords = set(stopwords.words("english"))
#words = word_tokenize(meaningful_text)

print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")

#assess this master and create a "related words" appendix to gradually make our keyword index better
#easily done next step is to have this call keywords from a DB and update the DB
print(gensim.summarization.keywords(meaningful_text))
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")
inputwords.append(word_tokenize(gensim.summarization.keywords(meaningful_text)))
print(inputwords)

#summarize the master overall document using gensim
print(gensim.summarization.summarize(meaningful_text)) 
#this last line of code isn't working because meaningful_text is a single string.  I am having problems
#turning the single string into an array of strings where each entry in the array is an individual sentences.
#if we can that figured out, gensim will then summarize

