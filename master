
import pandas as pd

import numpy as np

import nltk

from nltk.tokenize import sent_tokenize

from nltk.tokenize import word_tokenize

import nltk.corpus

from nltk.corpus import wordnet

from fuzzywuzzy import fuzz, process

import hashlib

#establishkeywords
#keyword index is initially established by the developer/user

keywords = ["politics", "economics", "treason", "slaves"]
searchwords = [keywords]

print("You have identified the following of interest:")
print(keywords)
print("----------------------------------------------------------")


#build out synonyms/antonyms of keywords
#we are going to use the package pyenchant to get synonyms and antonyms
#Importing the nltk wordnet, will give suggested alternate words if spelling off and syn/ant

synonyms = []
antonyms = []

for k in keywords:
    for syns in wordnet.synsets(k):
        #print(syns[0].name())
        #print(syns[0].lemmas()[0].name())
        #print(syns[0].definition())
        #print(syns[0].examples())
        for l in syns.lemmas():
            synonyms.append(l.name())
  
            if l.antonyms():
                antonyms.append(l.antonyms()[0].name())
    searchwords = [searchwords + synonyms + antonyms]        
    
    print("----------------------------------------------------------")

print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("I've taken the liberty to also search these common synonyms and antonyms: ")
print(searchwords)
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")

#open file and tokenize
#To Do: Open and read all files in a folder and tokenize instead of one file at a time
f=open("Text/natsrvc.txt").read()
sentences=sent_tokenize(f)

for s in searchwords:
    term = str(s)
    print("---break---", )
    fuzzyset = process.extractBests(term, sentences, score_cutoff=40, limit=100)
    print(fuzzyset)
    print("---break---")
    
#to consider: grab sentence before, and sentence after

#to do: append keyword related summary into a master overall document

#to consider: assess this master and create a "related words" appendix to gradually make our keyword index better

#summarize the master overall document using NLP

