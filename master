
import pandas as pd

import numpy as np

import nltk

from nltk.tokenize import sent_tokenize

from nltk.tokenize import word_tokenize

import nltk.corpus

from nltk.corpus import wordnet, stopwords

from fuzzywuzzy import fuzz, process

import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import gensim

from gensim.summarization import summarize

from gensim.summarization import keywords

from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, strip_tags

import json


inputwords = ["politics", "conservative", "values", "challenge"]

f=open("Text/natsrvc.txt").read()
#open file and tokenize
#To Do: Open and read all files in a folder and tokenize instead of one file at a time

print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")

searchwords = [inputwords]

synonyms = []
antonyms = []

for k in inputwords:
    for syns in wordnet.synsets(k):
        for l in syns.lemmas():
            synonyms.append(l.name())
  
            if l.antonyms():
                antonyms.append(l.antonyms()[0].name())
    searchwords = [searchwords + synonyms + antonyms]        

print("----------------------------------------------------------")
print("----------------------------------------------------------")
print("----------------------------------------------------------")
print(searchwords)

sentences=sent_tokenize(f)

#allow user to lower score_cutoff for their searchwords to have higher rate of hit
#by splitting can also have more robust summary
for s in searchwords:
    term = str(s)
    fuzzyset = process.extractBests(term, sentences, score_cutoff=30, limit=100)

#now search based on top 10 keywords *can be revised
relevant_words = keywords(f, words=10, split=True)

for r in relevant_words:
    term = str(r)
    keyset = process.extractBests(term, sentences, score_cutoff=80, limit=100)

fuzz_text=str(fuzzyset)
key_text=str(keyset)
clean_text= (fuzz_text + " " + key_text)
print(clean_text)

def myreplace(mystr):
    thetext = mystr.replace("\u2019","'").replace("\u201c","\"").replace("\u201d","\"")
    thetext = thetext.replace("\u2018","'").replace("\u2013","-").replace("\u2022","\"")
    thetext = thetext.replace("\u000d",chr(13))
    thetext = thetext.replace(chr(10)," ")
    thetext = thetext.replace(chr(39),"''")
    return thetext

clean_text = gensim.parsing.preprocessing.strip_multiple_whitespaces(clean_text)
#clean_text = gensim.parsing.preprocessing.strip_non_alphanum(clean_text)
clean_text = gensim.parsing.preprocessing.strip_tags(clean_text)
clean_text = myreplace(clean_text).replace("\\n"," ")


g=open("Text/master.txt", "r+")

g.write(clean_text)

print(clean_text)
#next step is cleaning out the token scores, parsing into >10 sentences, then using gensim summarization
